# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wx3sm9YkhykZ47ACi-eAhAIZzV3Zg6tF

Setup and Installation
"""

# Install required libraries
!pip install kagglehub
!pip install scikit-learn-extra  # For additional clustering algorithms

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('default')
sns.set_palette("viridis")

"""Load Dataset from Kaggle"""

# Load the dataset directly from Kaggle
import kagglehub

# Download the dataset
path = kagglehub.dataset_download("vjchoudhary7/customer-segmentation-tutorial-in-python")
print("Dataset downloaded to:", path)

# Find the CSV file
import os
csv_file = None
for file in os.listdir(path):
    if file.endswith('.csv'):
        csv_file = os.path.join(path, file)
        break

if csv_file:
    print(f"Loading dataset from: {csv_file}")
    df = pd.read_csv(csv_file)
else:
    # If dataset not found, load from alternative source
    print("CSV file not found in dataset directory. Loading from alternative source...")
    url = "https://raw.githubusercontent.com/krishnaik06/Mall-Customers-Segmentation/main/Mall_Customers.csv"
    df = pd.read_csv(url)

# Display dataset info
print("Dataset shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset information:")
print(df.info())
print("\nSummary statistics:")
print(df.describe())

"""Data Exploration and Preprocessing"""

# Check for missing values
print("Missing values:")
print(df.isnull().sum())

# Rename columns for easier handling
df.rename(columns={
    'Annual Income (k$)': 'Income',
    'Spending Score (1-100)': 'SpendingScore',
    'Age': 'Age',
    'CustomerID': 'CustomerID',
    'Gender': 'Gender'
}, inplace=True)

# Visualize the distribution of key features
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Income distribution
axes[0, 0].hist(df['Income'], bins=15, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('Income Distribution')
axes[0, 0].set_xlabel('Annual Income (k$)')
axes[0, 0].set_ylabel('Frequency')

# Spending score distribution
axes[0, 1].hist(df['SpendingScore'], bins=15, edgecolor='black', alpha=0.7)
axes[0, 1].set_title('Spending Score Distribution')
axes[0, 1].set_xlabel('Spending Score (1-100)')
axes[0, 1].set_ylabel('Frequency')

# Age distribution
axes[1, 0].hist(df['Age'], bins=15, edgecolor='black', alpha=0.7)
axes[1, 0].set_title('Age Distribution')
axes[1, 0].set_xlabel('Age')
axes[1, 0].set_ylabel('Frequency')

# Gender distribution
gender_counts = df['Gender'].value_counts()
axes[1, 1].bar(gender_counts.index, gender_counts.values, alpha=0.7)
axes[1, 1].set_title('Gender Distribution')
axes[1, 1].set_xlabel('Gender')
axes[1, 1].set_ylabel('Count')

plt.tight_layout()
plt.show()

# Scatter plot of Income vs Spending Score
plt.figure(figsize=(10, 6))
plt.scatter(df['Income'], df['SpendingScore'], alpha=0.6)
plt.title('Income vs Spending Score')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.grid(True, alpha=0.3)
plt.show()

# Prepare data for clustering
X = df[['Income', 'SpendingScore']].values

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Data prepared and scaled for clustering")

"""Determine Optimal Number of Clusters"""

# Use the Elbow Method to find the optimal number of clusters
wcss = []  # Within-Cluster Sum of Square
silhouette_scores = []

# Try different numbers of clusters
cluster_range = range(2, 11)

for n_clusters in cluster_range:
    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

    # Calculate silhouette score
    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)
    silhouette_scores.append(silhouette_avg)
    print(f"For n_clusters = {n_clusters}, silhouette score = {silhouette_avg:.4f}")

# Plot the Elbow Method graph
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(cluster_range, wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='--')
plt.title('Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Determine optimal number of clusters (where the elbow occurs)
optimal_clusters = 5  # Based on the elbow method and silhouette score
print(f"Optimal number of clusters: {optimal_clusters}")

""" Apply K-Means Clustering"""

# Apply K-Means with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42, n_init=10)
kmeans.fit(X_scaled)

# Add cluster labels to the original dataframe
df['Cluster'] = kmeans.labels_

# Visualize the clusters
plt.figure(figsize=(10, 6))
colors = ['red', 'blue', 'green', 'cyan', 'magenta', 'yellow', 'black', 'orange', 'purple', 'brown']

for i in range(optimal_clusters):
    plt.scatter(X[df['Cluster'] == i, 0], X[df['Cluster'] == i, 1],
                s=50, c=colors[i], label=f'Cluster {i}', alpha=0.7)

# Plot the centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='yellow', marker='*', label='Centroids', edgecolor='black')

plt.title('Customer Segments using K-Means Clustering')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Analyze the clusters
cluster_summary = df.groupby('Cluster').agg({
    'Income': ['mean', 'std', 'min', 'max'],
    'SpendingScore': ['mean', 'std', 'min', 'max'],
    'CustomerID': 'count'
}).round(2)

cluster_summary.columns = ['Income_Mean', 'Income_Std', 'Income_Min', 'Income_Max',
                          'SpendingScore_Mean', 'SpendingScore_Std', 'SpendingScore_Min', 'SpendingScore_Max',
                          'Count']

print("Cluster Summary:")
print(cluster_summary)

"""Try DBSCAN Clustering"""

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Add DBSCAN labels to the dataframe
df['DBSCAN_Cluster'] = dbscan_labels

# Count the number of clusters (excluding noise points labeled as -1)
n_dbscan_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)

print(f"DBSCAN identified {n_dbscan_clusters} clusters with {n_noise} noise points")

# Visualize DBSCAN clusters
plt.figure(figsize=(10, 6))

# Create a list of unique labels
unique_labels = set(dbscan_labels)

# Create colors for each cluster + noise
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise
        col = [0, 0, 0, 1]
        label = 'Noise'
    else:
        label = f'Cluster {k}'

    class_member_mask = (dbscan_labels == k)
    xy = X[class_member_mask]
    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=[col], label=label, alpha=0.7)

plt.title('Customer Segments using DBSCAN Clustering')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Compare DBSCAN with K-Means
dbscan_summary = df[df['DBSCAN_Cluster'] != -1].groupby('DBSCAN_Cluster').agg({
    'Income': ['mean', 'std', 'min', 'max'],
    'SpendingScore': ['mean', 'std', 'min', 'max'],
    'CustomerID': 'count'
}).round(2)

dbscan_summary.columns = ['Income_Mean', 'Income_Std', 'Income_Min', 'Income_Max',
                         'SpendingScore_Mean', 'SpendingScore_Std', 'SpendingScore_Min', 'SpendingScore_Max',
                         'Count']

print("DBSCAN Cluster Summary (excluding noise):")
print(dbscan_summary)

""" Bonus - Analyze Average Spending per Cluster"""

# Analyze average spending per cluster (K-Means)
plt.figure(figsize=(12, 5))

# Average spending by cluster
plt.subplot(1, 2, 1)
cluster_spending = df.groupby('Cluster')['SpendingScore'].mean().sort_values(ascending=False)
cluster_spending.plot(kind='bar', color='skyblue')
plt.title('Average Spending Score by Cluster (K-Means)')
plt.xlabel('Cluster')
plt.ylabel('Average Spending Score')
plt.xticks(rotation=0)
plt.grid(True, alpha=0.3)

# Average income by cluster
plt.subplot(1, 2, 2)
cluster_income = df.groupby('Cluster')['Income'].mean().sort_values(ascending=False)
cluster_income.plot(kind='bar', color='lightcoral')
plt.title('Average Income by Cluster (K-Means)')
plt.xlabel('Cluster')
plt.ylabel('Average Income (k$)')
plt.xticks(rotation=0)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Create customer profiles for each cluster
cluster_profiles = df.groupby('Cluster').agg({
    'Income': 'mean',
    'SpendingScore': 'mean',
    'Age': 'mean',
    'Gender': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown',
    'CustomerID': 'count'
}).round(2)

cluster_profiles.columns = ['Avg_Income', 'Avg_SpendingScore', 'Avg_Age', 'Most_Common_Gender', 'Count']

print("Customer Profiles by Cluster:")
print(cluster_profiles)

# Interpret the clusters
cluster_descriptions = {
    0: "High Income, Low Spending - Should be targeted with premium products",
    1: "Medium Income, Medium Spending - Average customers",
    2: "Low Income, High Spending - Budget-conscious but high spenders",
    3: "Low Income, Low Spending - Price-sensitive customers",
    4: "High Income, High Spending - Premium customers, most valuable segment"
}

print("\nCluster Interpretations:")
for cluster_id, description in cluster_descriptions.items():
    if cluster_id in cluster_profiles.index:
        print(f"Cluster {cluster_id}: {description}")

"""2D Visualization with Additional Features"""

# Part 8: 2D Visualization with Additional Features

# Prepare data with two features
X_2d = df[['Income', 'SpendingScore']].values
X_2d_scaled = StandardScaler().fit_transform(X_2d)

# Apply K-Means to 2D data
kmeans_2d = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42, n_init=10)
kmeans_2d.fit(X_2d_scaled)
df['Cluster_2D'] = kmeans_2d.labels_

# Create enough colors for all clusters
colors = plt.cm.tab10.colors  # Using tab10 colormap which has 10 distinct colors

# 2D plot
plt.figure(figsize=(12, 8))

# Get unique cluster labels from 2D clustering
unique_clusters_2d = df['Cluster_2D'].unique()

for i, cluster_id in enumerate(unique_clusters_2d):
    cluster_data = X_2d[df['Cluster_2D'] == cluster_id]
    plt.scatter(cluster_data[:, 0],
                cluster_data[:, 1],
                s=50, c=[colors[i % len(colors)]], label=f'Cluster {cluster_id}', alpha=0.7)

plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('2D Customer Segmentation (Income vs Spending Score)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Analyze 2D clusters
cluster_2d_summary = df.groupby('Cluster_2D').agg({
    'Income': ['mean', 'std', 'min', 'max'],
    'SpendingScore': ['mean', 'std', 'min', 'max'],
    'Age': ['mean', 'std', 'min', 'max'],
    'CustomerID': 'count'
}).round(2)

cluster_2d_summary.columns = ['Income_Mean', 'Income_Std', 'Income_Min', 'Income_Max',
                             'SpendingScore_Mean', 'SpendingScore_Std', 'SpendingScore_Min', 'SpendingScore_Max',
                             'Age_Mean', 'Age_Std', 'Age_Min', 'Age_Max',
                             'Count']

print("2D Cluster Summary:")
print(cluster_2d_summary)

# Compare original vs 2D clustering
print("\nComparison of original and 2D clustering results:")
comparison = df.groupby('Cluster').agg({
    'Cluster_2D': lambda x: x.mode()[0] if len(x.mode()) > 0 else -1
})
print(comparison)

# Create a cross-tabulation to see how original and 2D clusters relate
cross_tab = pd.crosstab(df['Cluster'], df['Cluster_2D'])
print("\nCross-tabulation of original vs 2D clusters:")
print(cross_tab)