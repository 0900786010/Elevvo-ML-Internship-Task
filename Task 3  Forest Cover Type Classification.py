# -*- coding: utf-8 -*-
"""Task 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d3Qkqvz0f3mdLyx373UU3dQ8TGiWHUHN

Setup and Data Loading
"""

# Install required packages (if not already installed)
!pip install xgboost
!pip install plotly

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz"
column_names = [
    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',
    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',
    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',
    'Horizontal_Distance_To_Fire_Points'
] + [f'Wilderness_Area_{i}' for i in range(1, 5)] + [f'Soil_Type_{i}' for i in range(1, 41)] + ['Cover_Type']

# Read the data
df = pd.read_csv(url, compression='gzip', header=None, names=column_names)

print("Dataset shape:", df.shape)
df.head()

"""Data Exploration and Preprocessing"""

# Basic information about the dataset
print("Dataset Info:")
print(df.info())
print("\nMissing values:\n", df.isnull().sum().sum())  # Should be 0

# Check the distribution of the target variable
print("\nCover Type Distribution:")
print(df['Cover_Type'].value_counts())

# Visualize the target distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='Cover_Type', data=df)
plt.title('Distribution of Forest Cover Types')
plt.xlabel('Cover Type')
plt.ylabel('Count')
plt.show()

# Separate features and target
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

# Standardize the numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for better visualization
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

"""Model Training and Evaluation"""

# Initialize models
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# XGBoost requires class labels to start from 0, so we need to adjust the labels
# Create a mapping from original labels to 0-indexed labels
label_mapping = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6}
y_train_xgb = y_train.map(label_mapping)
y_test_xgb = y_test.map(label_mapping)

xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')

# Train models
print("Training Random Forest...")
rf_model.fit(X_train, y_train)

print("Training XGBoost...")
xgb_model.fit(X_train, y_train_xgb)

# Make predictions
rf_pred = rf_model.predict(X_test)
xgb_pred = xgb_model.predict(X_test)

# Convert XGBoost predictions back to original labels
reverse_mapping = {v: k for k, v in label_mapping.items()}
xgb_pred_original = np.vectorize(reverse_mapping.get)(xgb_pred)

# Evaluate models
rf_accuracy = accuracy_score(y_test, rf_pred)
xgb_accuracy = accuracy_score(y_test, xgb_pred_original)

print(f"Random Forest Accuracy: {rf_accuracy:.4f}")
print(f"XGBoost Accuracy: {xgb_accuracy:.4f}")

# Classification reports
print("\nRandom Forest Classification Report:")
print(classification_report(y_test, rf_pred))

print("\nXGBoost Classification Report:")
print(classification_report(y_test, xgb_pred_original))

"""Visualization - Confusion Matrix"""

# Create confusion matrices
rf_cm = confusion_matrix(y_test, rf_pred)
xgb_cm = confusion_matrix(y_test, xgb_pred)

# Plot confusion matrices
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Random Forest confusion matrix
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Random Forest Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# XGBoost confusion matrix
sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('XGBoost Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()

"""Feature Importance"""

# Get feature importances
rf_importances = rf_model.feature_importances_
xgb_importances = xgb_model.feature_importances_

# Create DataFrames for visualization
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'RF_Importance': rf_importances,
    'XGB_Importance': xgb_importances
})

# Sort by Random Forest importance
rf_sorted = feature_importance_df.sort_values('RF_Importance', ascending=False).head(15)

# Sort by XGBoost importance
xgb_sorted = feature_importance_df.sort_values('XGB_Importance', ascending=False).head(15)

# Plot feature importances
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

# Random Forest feature importance
sns.barplot(x='RF_Importance', y='Feature', data=rf_sorted, ax=axes[0])
axes[0].set_title('Top 15 Features - Random Forest')
axes[0].set_xlabel('Importance')

# XGBoost feature importance
sns.barplot(x='XGB_Importance', y='Feature', data=xgb_sorted, ax=axes[1])
axes[1].set_title('Top 15 Features - XGBoost')
axes[1].set_xlabel('Importance')

plt.tight_layout()
plt.show()

"""Final Comparison and Results"""

# Create a comparison table with just the baseline models
results = pd.DataFrame({
    'Model': ['Random Forest', 'XGBoost'],
    'Accuracy': [rf_accuracy, xgb_accuracy]
})

print("Model Comparison:")
print(results)

# Visual comparison
plt.figure(figsize=(8, 6))
sns.barplot(x='Model', y='Accuracy', data=results)
plt.title('Model Accuracy Comparison')
plt.ylim(0.9, 1.0)  # Adjust based on your results
plt.show()

# Print the best model
best_model_name = results.loc[results['Accuracy'].idxmax(), 'Model']
best_accuracy = results.loc[results['Accuracy'].idxmax(), 'Accuracy']
print(f"\nBest model: {best_model_name} with accuracy {best_accuracy:.4f}")